library(Metrics)
library(ISLR)
library(h2o)
require(MASS)
library(CASdatasets)
require(stats)
library(data.table)
library(plyr)
library(rpart)
library(rpart.plot)
library(Hmisc)
library(glmnet)
library(dplyr)
rm(list = ls())
# install.packages("installr"); library(installr)
#### Library for Severity model 
# install.packages("devtools")
# devtools::install_github('henckr/distRforest')
library(distRforest)
# install.packages("rfCountData")
# version
 # install.packages("remotes")
 # remotes::install_github("fpechon/rfCountData")
library(rfCountData)
#####
testyear1=pg17testyear1
testyear2=pg17testyear2
testyear3=pg17testyear3
testyear4=pg17testyear4
# a1=pg17trainpol
# a2=pg17trainclaim
### Data cleaning done in excel, removed -ve claims amount and aggregated data and brought back the cleaned data below: 
dat= read.csv("C:/Users/tisra/Desktop/Phd Code/11. APTP by factor/New Data set Pricing Game/R Input/Train_data_upload.csv")
##### Adding Exclusions to the severity data #####
dat_adj=dat[which(dat$ClaimAmount==0 |  dat$ClaimAmount>= 84.42),]
dat_adj= dat_adj[which(dat_adj$ClaimAmount<=16770.62),]
dat_adj= dat_adj[which(!dat_adj$ClaimAmount %in% c(56.19,60.29,75.88,103.59,106.14,110.98,130.11,618,1010.41,1236,1240.89,1442.87,2323.95,2453.28,2683.76
)),]
dat_adj= dat_adj[which(!dat_adj$id_policy %in% c("A00000765-V02")),]  ## Policy removed as did not have Vehicle Age 
dat_adj$ClaimNb = ifelse(dat_adj$ClaimNb > 3, 3, dat_adj$ClaimNb)

#### Character data only 
char_cols <- sapply(dat_adj, is.character)
dat_adj_char= dat_adj[,char_cols]
dat_adj_char[] <- lapply(dat_adj_char, as.factor)
dat_adj_char=dat_adj_char[,5:16]
dat_adj_char=dat_adj_char[,c(1:4,6:10,12)]
#### Numeric data only 
num_cols <- sapply(dat_adj, is.numeric)
dat_adj_num <- dat_adj[, num_cols]
dat_adj_boxplot= cbind(dat_adj_char,dat_adj_num)
dat_adj_boxplot$exposure =1
dat_adj_boxplot$IDpol= dat_adj$id_policy
##### Shuffling the data
dat_freq=dat_adj_boxplot
n_freq = nrow(dat_freq)
set.seed(100)
reord_freq = sample(1:n_freq, size=n_freq, replace=FALSE)
dat_freq = dat_freq[reord_freq, ] 

# save a validation sample for later:
set.seed(100)
split1= sample(1:n_freq, size=.833*n_freq, replace=FALSE)
CV_data_freq = dat_freq[split1,]
test_data_freq = dat_freq[-split1,]
dev_poiss <- function(ytrue, ypred, wcase) -2 * weighted.mean(dpois(ytrue, ypred, log = TRUE) - dpois(ytrue, ytrue, log = TRUE), wcase, na.rm = TRUE)
dev_gamma <- function(ytrue, ypred, wcase) -2 * weighted.mean(log(ytrue/ypred) - ((ytrue - ypred) / ypred), wcase, na.rm = TRUE)
###### creating the grid
ncand=rep(2:4, times=1,each=1)
ntrees= rep(1000,3)
grid=cbind(ncand,ntrees)

####################### CV for frequency forest #########
K=5
n = nrow(CV_data_freq)
folds = cut(1:n, K, labels=FALSE)
err.vinternal.freq=err.vout.freq= numeric(3)
err.cv.in=err.cv.out=matrix("NA",3,5)
cp.opt=numeric(5)
err.cv.in.fin=err.cv.out.fin=matrix(NA,5,10)

p=10

 for (t in 1:p) {
  ## re shuffling CV data
 n_CV_freq = nrow(CV_data_freq)
  reord_CVfreq = sample(1:n_CV_freq, size=n_CV_freq, replace=FALSE)
  CV_data_freq = CV_data_freq[reord_CVfreq, ]
  
 for (a in 1:K) {
  i = which(folds==a)
  vtrain.data.freq=CV_data_freq[-i,]
  valid.data.freq=CV_data_freq[i,]
  Ntr = nrow(vtrain.data.freq)
  Ktr = 5
  err.out.freq = numeric(3)
  for (c in 1:3) {# tuning tree model (equivalent to cv.glmnet())

    tr.folds = cut(1:Ntr, Ktr, labels=FALSE)
    pdev.errors.tr = numeric(Ktr)
    for(k in 1:Ktr){

      ik = which(tr.folds==k)
      vtrain.data.freq.tr = vtrain.data.freq[-ik,]
      valid.data.freq.tr = vtrain.data.freq[ik,]
      forest_freq_valid.tr <-rfPoisson(y=vtrain.data.freq.tr$ClaimNb, offset = log(vtrain.data.freq.tr$Exposure),
            x=vtrain.data.freq.tr[c("Area", "VehPower","VehAge","DrivAge","BonusMalus","VehBrand","VehGas","Density","Region")],mtry=grid[c,1],ntree= grid[c,2],nodesize = 10000)

      valid_pred <- predict(forest_freq_valid.tr, newdata=valid.data.freq.tr,offset = log(valid.data.freq.tr$Exposure))
    pdev.errors.tr[k] = Poisson.Deviance(valid_pred, valid.data.freq.tr$ClaimNb)
    }
    err.out.freq[c] = mean(pdev.errors.tr) # mean CV error
  }
  err.cv.out[,a]=err.out.freq
  cp.opt[a] = which.min(err.cv.out)  
  forest_freq_cp_opt<-rfPoisson(y=vtrain.data.freq$ClaimNb, offset = log(vtrain.data.freq$Exposure),
                                x=vtrain.data.freq[c("pol_pay_freq","drv_drv2","drv_age_lic1","pol_payd","drv_sex1","vh_age","Density","pol_bonus","pol_coverage",
                    "pol_usage","vh_fuel","vh_type","vh_speed","pol_duration","vh_value","pol_sit_duration",
                    "vh_weight","drv_age1")],mtry=grid[cp.opt[a],1],ntree= grid[cp.opt[a],2],nodesize = 1000)
  
  
  valid_pred_fin <- predict(forest_freq_cp_opt, newdata=valid.data.freq,offset = log(valid.data.freq$Exposure))
  err.cv.out.fin[a,t]=dev_poiss(valid.data.freq$ClaimNb,valid_pred_fin,valid.data.freq$Exposure)
  
  ## Internal Error 
  internal_pred_fin <- predict(forest_freq_cp_opt, newdata=vtrain.data.freq,offset = log(vtrain.data.freq$Exposure))
  err.cv.in.fin[a,t]=dev_poiss(vtrain.data.freq$ClaimNb,internal_pred_fin,vtrain.data.freq$Exposure)
}
}  
 
write.csv(err.cv.out.fin,"err.cv.out.fin.csv")
write.csv(err.cv.in.fin,"err.cv.in.fin.csv")
###### End of CV for frequency ########################

# (1) Prediction performance from CV:
mean(err.cv.out.fin)
# (2) Independent test error evaluation:
final.cp.opt.test=apply(err.cv.out,2,which.min)
final.cp.opt=round(mean(final.cp.opt.test))

forest_freq <-rfPoisson(y=CV_data_freq$ClaimNb, offset = log(CV_data_freq$Exposure),
                        x=CV_data_freq[c("pol_pay_freq","drv_drv2","drv_age_lic1","pol_payd","drv_sex1","vh_age","Density","pol_bonus","pol_coverage",
                     "pol_usage","vh_fuel","vh_type","vh_speed","pol_duration","vh_value","pol_sit_duration",
                       "vh_weight","drv_age1")],mtry=grid[final.cp.opt,1],ntree=grid[final.cp.opt,2],nodesize = 1000)


summary(forest_freq)
##### Learn and test Errors/ Deviance 
varImpPlot(forest_freq)
varImp(forest_freq)
imp_freq=importance(forest_freq)
write.csv(imp_freq,"imp_freq.csv")
pred_internal_freq= predict(forest_freq, CV_data_freq, offset = log(CV_data_freq$Exposure))
pred_external_freq=predict(forest_freq,test_data_freq, offset = log(test_data_freq$Exposure))

err.internal.fin.freq=dev_poiss(CV_data_freq$ClaimNb,pred_internal_freq,CV_data_freq$Exposure)
err.out.fin.freq=dev_poiss(test_data_freq$ClaimNb,pred_external_freq,test_data_freq$Exposure)

final_frequency_error=cbind(err.internal.fin.freq,err.out.fin.freq)
write.csv(final_frequency_error,"final_frequency_error.csv")

######### Boot 632 for external error ######
n_freq=nrow(test_data_freq)
boot632_err_outfreq=err_oob_outfreq=inbag_err_freqout=numeric(250)

for (t in 1:250) {
  ib = sample(1:n_freq, size=1*n_freq, replace=TRUE)
  inbag_sample_outfreq=test_data_freq[ib,]
  
  pred_inbag_outfreq <- predict(forest_freq, newdata=inbag_sample_outfreq, offset = log(inbag_sample_outfreq$Exposure))
  inbag_err_freqout[t] =dev_poiss(inbag_sample_outfreq$ClaimNb,pred_inbag_outfreq,inbag_sample_outfreq$Exposure)
  
  #### oob error prediction
  
  oob_outfreq= which(!(test_data_freq$IDpol %in% inbag_sample_outfreq$IDpol))     
  oob_sample_outfreq= test_data_freq[oob_outfreq,]
  
  pred_oob_outfreq <- predict(forest_freq, newdata=oob_sample_outfreq,offset = log(oob_sample_outfreq$Exposure))
  err_oob_outfreq[t] =dev_poiss(oob_sample_outfreq$ClaimNb,pred_oob_outfreq,oob_sample_outfreq$Exposure)
  
  boot632_err_outfreq [t] = 0.368 * err_oob_outfreq[t] + 0.632 * inbag_err_freqout[t]
  
}
bootstrap_freqout=cbind(boot632_err_outfreq,err_oob_outfreq,inbag_err_freqout)
write.csv(bootstrap_freqout,"bootstrap_freqout.csv")
range_freqout= cbind(quantile(boot632_err_outfreq,0.025),quantile(boot632_err_outfreq,0.975))

###### severity Model ###################################################################################################
###### creating the grid
ncand_sev=rep(2:4, times=1,each=1)
ntrees= rep(1000,3)
grid=cbind(ncand,ntrees)
#############################################################################
dat_severity=dat_freq[dat_freq$ClaimAmount!=0,]
n_severity = nrow(dat_severity)
set.seed(1001)
reord_sev = sample(1:n_severity, size=n_severity, replace=FALSE)
dat_severity = dat_severity[reord_sev, ] 
dat_severity$AverageClaim= dat_severity$ClaimAmount/ dat_severity$ClaimNb
# save a validation sample for later:
set.seed(100)
i.valid_sev = sample(1:n_severity, size=.833*n_severity, replace=FALSE)
CV_data_sev = dat_severity[i.valid_sev,]
test_data_sev = dat_severity[-i.valid_sev,]

############ CV loop for Severity model 
K=5
cp.opt=numeric(5)
n_sev = nrow(CV_data_sev)
folds_sev = cut(1:n_sev, K, labels=FALSE)
err.out.sev=err.vinternal.sev=err.vout.sev= numeric(3)
err.cvin.sev=err.cvout.sev=matrix("NA",3,5)
err.cvin.fin.sev=err.cvout.fin.sev=matrix(NA,5,10)
p=10

for (n in 1:p) {
  ## re shuffling CV data
  n_CV_sev = nrow(CV_data_sev)
  reord_CVsev = sample(1:n_CV_sev, size=n_CV_sev, replace=FALSE)
  CV_data_sev = CV_data_sev[reord_CVsev,]

 for (a in 1:K) {
  i = which(folds_sev==a)
  vtrain.data.sev=CV_data_sev[-i,]
  valid.data.sev=CV_data_sev[i,]
  Ntr = nrow(vtrain.data.sev)
  Ktr = 5
  for (c in 1:3) {
    tr.folds = cut(1:Ntr, Ktr, labels=FALSE)
    pdev.errors.tr = numeric(Ktr)
    for(k in 1:Ktr){

      ik = which(tr.folds==k)
      vtrain.data.sev.tr = vtrain.data.sev[-ik,]
      valid.data.sev.tr = vtrain.data.sev[ik,]
      forest_CV_sev.tr= distRforest::rforest(formula = as.formula('AverageClaim ~ Area+VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Density + Region'),
                                             data = vtrain.data.sev.tr, method = 'gamma', weights =ClaimNb,
                                             control = rpart.control(cp = 0.0001, minbucket = 500),ncand = grid[c,1], ntrees=grid[c,2])


      valid_pred <- predict(forest_CV_sev.tr, newdata=valid.data.sev.tr)
      pdev.errors.tr[k] = dev_gamma(valid.data.sev.tr$AverageClaim,valid_pred,valid.data.sev.tr$ClaimNb)
    }
    err.out.sev[c] = mean(pdev.errors.tr) # mean CV error
  }
  err.cvout.sev[,a]=err.out.sev

  cp.opt[a] = which.min(err.out.sev)
  forest_cp_out_sev= distRforest::rforest(formula = as.formula('AverageClaim ~ pol_pay_freq+drv_drv2+drv_age_lic1+pol_payd+drv_sex1+vh_age+Density+
                                            pol_bonus+pol_coverage+pol_usage+vh_fuel+vh_type+vh_speed+pol_duration+vh_value+pol_sit_duration+
                                                               vh_weight+drv_age1'),
                                          data = vtrain.data.sev, method = 'gamma', weights =ClaimNb,
                                          control = rpart.control(cp = 0.0001, minbucket = 100),ncand = grid[cp.opt[a],1], ntrees=grid[cp.opt[a],2])
  
  
  valid_pred_fin.sev <-predict.rforest(forest_cp_out_sev, newdata=valid.data.sev)
  err.cvout.fin.sev[a,n]=dev_gamma(valid.data.sev$AverageClaim,valid_pred_fin.sev,valid.data.sev$ClaimNb)
  
  ### internal error 
  internal_pred_fin.sev <- predict.rforest(forest_cp_out_sev, newdata=vtrain.data.sev)
  err.cvin.fin.sev[a,n]=dev_gamma(vtrain.data.sev$AverageClaim,internal_pred_fin.sev,vtrain.data.sev$ClaimNb)
  
} 
  # write.csv(err.cvout.sev,paste0(n,"Tunning Error.csv"))
}
 
write.csv(err.cvout.fin.sev,"err.cvout.fin.sev.csv")
write.csv(err.cvin.fin.sev,"err.cvin.fin.sev.csv")

# (2) Independent test error evaluation:
final.cp.opt.test.sev=apply(err.cvout.sev,2,which.min)
final.cp.opt.sev=round(mean(final.cp.opt.test.sev))

forest_sev= distRforest::rforest(formula = as.formula('AverageClaim ~ pol_pay_freq+drv_drv2+drv_age_lic1+pol_payd+drv_sex1+vh_age+Density+
                                            pol_bonus+pol_coverage+pol_usage+vh_fuel+vh_type+vh_speed+pol_duration+vh_value+pol_sit_duration+
                                                               vh_weight+drv_age1'),
                                 data = CV_data_sev, method = 'gamma', weights =ClaimNb,
                                 control = rpart.control(cp = 0.0001, minbucket = 100),ncand = grid[final.cp.opt.sev,1], ntrees = grid[final.cp.opt.sev,2])

imp_severity=importance_rforest(forest_sev)
write.csv(imp_severity,"imp_severity.csv")

pred_internal_sev= predict.rforest(forest_sev, CV_data_sev)
pred_external_sev=predict.rforest(forest_sev,test_data_sev)

err.fin.internal.sev=dev_gamma(CV_data_sev$AverageClaim,pred_internal_sev,CV_data_sev$ClaimNb)
err.fin.out.sev=dev_gamma(test_data_sev$AverageClaim,pred_external_sev,test_data_sev$ClaimNb)

final_severity_error= cbind(err.fin.internal.sev,err.fin.out.sev)
write.csv(final_severity_error,"final_severity_error.csv")

##### Bootstrap 632 for severity #######

n_sev=nrow(test_data_sev)
boot632_err_outsev=err_oob_outsev=inbag_err_sevout=numeric(250)

for (t in 1:250) {
  ib_sev = sample(1:n_sev, size=1*n_sev, replace=TRUE)
  inbag_sample_outsev=test_data_sev[ib_sev,]
  
  pred_inbag_outsev <- predict.rforest(forest_sev, newdata=inbag_sample_outsev)
  inbag_err_sevout[t] =dev_gamma(inbag_sample_outsev$AverageClaim,pred_inbag_outsev,inbag_sample_outsev$ClaimNb)
  
  #### oob error prediction
  
  oob_outsev= which(!(test_data_sev$IDpol %in% inbag_sample_outsev$IDpol))     
  oob_sample_outsev= test_data_sev[oob_outsev,]
  
  pred_oob_outsev <- predict.rforest(forest_sev, newdata=oob_sample_outsev)
  err_oob_outsev[t] =dev_gamma(oob_sample_outsev$AverageClaim,pred_oob_outsev,oob_sample_outsev$ClaimNb)
  
  boot632_err_outsev[t] = 0.368 * err_oob_outsev[t] + 0.632 * inbag_err_sevout[t]
}

bootstrap_sevout=cbind(boot632_err_outsev,err_oob_outsev,inbag_err_sevout)
write.csv(bootstrap_sevout,"bootstrap_sevout.csv")
range_sevout= cbind(quantile(boot632_err_outsev,0.025),quantile(boot632_err_outsev,0.975))

####### Complete premium values to check how the model performs overall
####### Taking the original Frequency test data to get the final prediction of premium#################
test_data_freq
pred_external_freq   ### Predicted claims frequency 
Pred_severity <- predict.rforest(forest_sev, newdata=test_data_freq) #### Predicted Severity Final 
orignal_data= test_data_freq$ClaimAmount
a=sum(orignal_data)
predicted_premium= Pred_severity*pred_external_freq*test_data_freq$Exposure
b=sum(predicted_premium)
error_prem= (a-b)/a
write.csv(test_data_freq,"test_data_freq.csv")
write.csv(predicted_premium,"predicted_premium.csv")
write.csv(orignal_data,"orignal_data.csv")

####### Bootstrap 632 for bias ##########################################################
n = nrow(test_data_freq)
B = 250
pred_inbag=Actual_inbag=bias_inbag=percent.bias_inbag=pred_oob= Actual_oob= bias_oob= percent.bias_oob=boot632_bias= boot632_bias_perc=numeric(B)

for(b in 1:B){
  ib_bias = sample(1:n, size=1*n, replace=TRUE)
  
  ####### in sample bias calculation
  inbag_sample_bias=test_data_freq[ib_bias,]
  
  #### Freq for bias################################################################
  pred_freq_inbag <- predict(forest_freq, newdata=inbag_sample_bias, offset = log(inbag_sample_bias$Exposure))
  ### Severity part of the bias
  Pred_severity_inbag <- predict.rforest(forest_sev, newdata=inbag_sample_bias) #### Predicted Severity Final
  ### comparison step
  orignal_data_inbag= inbag_sample_bias$ClaimAmount
  
  predicted_premium_inbag= pred_freq_inbag*Pred_severity_inbag*inbag_sample_bias$Exposure
  
  pred_inbag[b]=sum(predicted_premium_inbag)
  Actual_inbag[b]= sum(orignal_data_inbag)
  
  bias_inbag[b] = Actual_inbag[b]-pred_inbag[b]
  percent.bias_inbag[b]=bias_inbag[b]/ Actual_inbag[b]
  
  ##### Out of bag bias evaluation ############################################ 
  
  oob_sample= which(!(test_data_freq$IDpol %in% inbag_sample_bias$IDpol))
  oob_sample_bias= test_data_freq[oob_sample,]
  
  #### Freq for bias###################################
  
  pred_freq_oob <- predict(forest_freq, newdata=oob_sample_bias,offset = log(oob_sample_bias$Exposure))
  ### Severity part of the bias
  Pred_severity_oob <- predict.rforest(forest_sev, newdata=oob_sample_bias) #### Predicted Severity Final
  ### comparison step
  orignal_data_oob= oob_sample_bias$ClaimAmount
  
  predicted_premium_oob= pred_freq_oob*Pred_severity_oob*oob_sample_bias$Exposure
  
  pred_oob[b]=sum(predicted_premium_oob)
  Actual_oob[b]= sum(orignal_data_oob)
  
  bias_oob[b] = Actual_oob[b]-pred_oob[b]
  percent.bias_oob[b]=bias_oob[b]/ Actual_oob[b]
  
  ########### Final 632 errors to be used #####################################
  
  boot632_bias[b] = 0.368 * bias_oob[b] + 0.632 * bias_inbag[b]
  
  boot632_bias_perc[b] = 0.368 * percent.bias_oob[b] + 0.632 * percent.bias_inbag[b]
  
}

bootstrap_bias=cbind(bias_inbag,percent.bias_inbag,bias_oob,percent.bias_oob,boot632_bias, boot632_bias_perc)
write.csv(bootstrap_bias,"bootstrap_bias.csv")
View(bootstrap_bias)


