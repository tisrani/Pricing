library(xgboost)
require(MASS)
library(CASdatasets)
require(stats)
library(data.table)
library(plyr)
library(rpart)
library(rpart.plot)
library(pdp)           # for partial dependence plots
library(vip)           # for variable importance plots
library(Hmisc)
library(glmnet)
library(dplyr)
library("wesanderson")
library(statmod)
library(scales)
library(xgboost)
library(locfit)
library(Metrics)
library(ISLR)
library(h2o)
library(tidyr)
library(tibble) 
library(insurancerating)
library(evtree)
library(mgcv)
require(MASS)
library(CASdatasets)
require(stats)
library(data.table)
# library(plyr)
library(dplyr)
library(purrr)
library(rpart)
library(rpart.plot)
library(Hmisc)
library(glmnet)
library(dplyr)
data(freMTPL2freq)
data(freMTPL2sev)
library("wesanderson")
library(statmod)
library(scales)
data(pg17trainpol)
data(pg17trainclaim)
data(pg17testyear1)
data(pg17testyear2)
data(pg17testyear3)
data(pg17testyear4)
data(fredpt17)
library(ggcorrplot)
library(ggplot2)

####### Importing data 
# trainpol=pg17trainpol
# trainclaim=pg17trainclaim
testyear1=pg17testyear1
testyear2=pg17testyear2
testyear3=pg17testyear3
testyear4=pg17testyear4
###### Importing aggregated Freq severity in the training data 
dat= read.csv("C:/Users/tisra/Desktop/Phd Code/11. APTP by factor/New Data set Pricing Game/R Input/Train_data_upload.csv")
##### Adding Exclusions to the severity data #####
dat_adj=dat[which(dat$ClaimAmount==0 |  dat$ClaimAmount>= 84.42),]
dat_adj= dat_adj[which(dat_adj$ClaimAmount<=16770.62),]
dat_adj= dat_adj[which(!dat_adj$ClaimAmount %in% c(56.19,60.29,75.88,103.59,106.14,110.98,130.11,618,1010.41,1236,1240.89,1442.87,2323.95,2453.28,2683.76
)),]
dat_adj= dat_adj[which(!dat_adj$id_policy %in% c("A00000765-V02")),]  ## Policy removed as did not have Vehicle Age 
dat_adj$ClaimNb = ifelse(dat_adj$ClaimNb > 3, 3, dat_adj$ClaimNb)

#### Character data only 
char_cols <- sapply(dat_adj, is.character)
dat_adj_char= dat_adj[,char_cols]
dat_adj_char[] <- lapply(dat_adj_char, as.factor)
dat_adj_char=dat_adj_char[,5:16]
dat_adj_char=dat_adj_char[,c(1:4,6:10,12)]
#### Numeric data only 
num_cols <- sapply(dat_adj, is.numeric)
dat_adj_num <- dat_adj[, num_cols]
dat_adj_boxplot= cbind(dat_adj_char,dat_adj_num)
dat_adj_boxplot$exposure =1

###### Banding the continuous variables,
##### Driver Age 1
table(dat_adj_boxplot$drv_age1)
drv_age1GLM <- cbind(c(19:101), c(rep("19-40",22), rep("41-67",27), rep("68-75",8), rep("76-82",7), rep("83-92",10), rep("93-101",9)))
dat_adj_boxplot$drv_age1 <- as.factor(drv_age1GLM[dat_adj_boxplot$drv_age1-18,2])
table(dat_adj_boxplot$drv_age1)
dat_adj_boxplot[,"drv_age1"] <-relevel(dat_adj_boxplot[,"drv_age1"], ref="41-67")

##### drv_age_lic1
table(dat_adj_boxplot$drv_age_lic1)
drv_age_lic1GLM <- cbind(c(1:111), c(rep("1-30",30), rep("31-43",13), rep("44-53",10), rep("54-111",58)))
dat_adj_boxplot$drv_age_lic1 <- as.factor(drv_age_lic1GLM[dat_adj_boxplot$drv_age_lic1,2])
table(dat_adj_boxplot$drv_age_lic1)
dat_adj_boxplot[,"drv_age_lic1"] <-relevel(dat_adj_boxplot[,"drv_age_lic1"], ref="1-30")

##### vh_age
table(dat_adj_boxplot$vh_age)
vh_ageGLM <- cbind(c(1:66), c(rep("1-10",10), rep("11-17",7), rep("18-26",9), rep("27-66",40)))
dat_adj_boxplot$vh_age <- as.factor(vh_ageGLM[dat_adj_boxplot$vh_age,2])
table(dat_adj_boxplot$vh_age)
dat_adj_boxplot[,"vh_age"] <-relevel(dat_adj_boxplot[,"vh_age"], ref="1-10")

##### vh_speed
table(dat_adj_boxplot$vh_speed)
vh_speedGLM <- cbind(c(25:310), c(rep("25-145",121), rep("146-154",9), rep("155-183",29), rep("184-285",102),
                                  rep("286-310",25)))
dat_adj_boxplot$vh_speed <- as.factor(vh_speedGLM[dat_adj_boxplot$vh_speed-24,2])
table(dat_adj_boxplot$vh_speed)
dat_adj_boxplot[,"vh_speed"] <-relevel(dat_adj_boxplot[,"vh_speed"], ref="155-183")

##### vh_value
table(dat_adj_boxplot$vh_value)
dat_adj_boxplot$vh_value[dat_adj_boxplot$vh_value==0]=1
vh_valueGLM <- cbind(c(0:155498), c(rep("0-17970",17971), rep("17971-18936",966), rep("18937-23950",5014), rep("23951-51520",27570),
                                    rep("51521-155498",103978)))

dat_adj_boxplot$vh_value <- as.factor(vh_valueGLM[dat_adj_boxplot$vh_value,2])
table(dat_adj_boxplot$vh_value)
dat_adj_boxplot[,"vh_value"] <-relevel(dat_adj_boxplot[,"vh_value"], ref="0-17970")

##### vh_weight
table(dat_adj_boxplot$vh_weight)
vh_weightGLM <- cbind(c(0:7901), c(rep("0-1500",1501), rep("1501-2000",500), rep("2001-2500",500), rep("2501-7901",5401)
))
dat_adj_boxplot$vh_weight[dat_adj_boxplot$vh_weight==0]=1
dat_adj_boxplot$vh_weight <- as.factor(vh_weightGLM[dat_adj_boxplot$vh_weight,2])
table(dat_adj_boxplot$vh_weight)
dat_adj_boxplot[,"vh_weight"] <-relevel(dat_adj_boxplot[,"vh_weight"], ref="0-1500")

##### Shuffling the data
dat_freq=dat_adj_boxplot
n_freq = nrow(dat_freq)
set.seed(100)
reord_freq = sample(1:n_freq, size=n_freq, replace=FALSE)
dat_freq = dat_freq[reord_freq, ] 
# save a validation sample for later:
set.seed(100)
split1= sample(1:n_freq, size=.833*n_freq, replace=FALSE)
CV_data_freq = dat_freq[split1,]
test_data_freq = dat_freq[-split1,]

dev_poiss <- function(ytrue, ypred, wcase) -2 * weighted.mean(dpois(ytrue, ypred, log = TRUE) - dpois(ytrue, ytrue, log = TRUE), wcase, na.rm = TRUE)
dev_gamma <- function(ytrue, ypred, wcase) -2 * weighted.mean(log(ytrue/ypred) - ((ytrue - ypred) / ypred), wcase, na.rm = TRUE)
##### GBM Model fitting
hyper_grid <- expand.grid(
  n.trees = c(2000),
  shrinkage = c(0.1,0.01,0.001),
  interaction.depth = c(1,2,3)
)
######## The Cross Validation loop goes in here  ####################????????????????????????????
n = nrow(train_data)
K = 5
folds = cut(1:n, K, labels=FALSE)
err.vinternal.freq=err.vout.freq= numeric(9)
err.cv.in=err.cv.out=matrix("NA",9,5)
pw.opt=cp.opt=err.internal=err.kfold =matrix(NA,5,1) 
p=10
set.seed(100)

for (b in 1:p) {
## re shuffling CV data
 n_CV_freq = nrow(train_data)
 reord_CVfreq = sample(1:n_CV_freq, size=n_CV_freq, replace=FALSE)
 CV_data_freq = train_data[reord_CVfreq, ]
  
for(t in 1:K){
 ##training sample
    i = which(folds==t)
    vtrain.data.freq=CV_data_freq[-i,]
    valid.data.freq=CV_data_freq[i,]
    Ntr = nrow(vtrain.data.freq)
    Ktr = 5
    err.out.freq = numeric(9)
    output.tr=matrix(NA,)
    pw_cv_out=int.output.tr=pred.output.tr=matrix(NA,5,9)
for (c in 1:9) {# tuning tree model (equivalent to cv.glmnet())
    tr.folds = cut(1:Ntr, Ktr, labels=FALSE)
    internal.errors.tr=pdev.errors.tr = numeric(Ktr)
for(k in 1:Ktr){
      ik = which(tr.folds==k)
      vtrain.data.freq.tr = vtrain.data.freq[-ik,]
      valid.data.freq.tr = vtrain.data.freq[ik,]
  ################### Extreme gradient boost matrix
      features.xgb.freq <- setdiff( 1:length(vtrain.data.freq.tr) ,  grep("drv_age_lic_avg|id_policy|drv_sex1|drv_sex2|vh_make|pol_bonus|pol_duration|pol_sit_duration|vh_make|drv_age2|drv_age_lic2|vh_din|vh_sale_begin|vh_sale_end|ClaimAmount|drv_age_avg|exposure|ClaimNb|
      drv_age_lic_avg|vh_cyl", names(vtrain.data.freq.tr)  ))

      xgb_vtrain.data.freq.tr <- xgb.DMatrix(data = data.matrix(vtrain.data.freq.tr [,features.xgb.freq]), label = vtrain.data.freq.tr$ClaimNb)
      xgb_valid.data.freq.tr <- xgb.DMatrix(data = data.matrix(valid.data.freq.tr[,features.xgb.freq]), label = valid.data.freq.tr$ClaimNb)

      setinfo(xgb_vtrain.data.freq.tr,"base_margin",log(vtrain.data.freq.tr$exposure))
      setinfo(xgb_valid.data.freq.tr,"base_margin",log(valid.data.freq.tr$exposure))

####################################  testing for the correct power   

      # train model for the internal loop 
      {t1 <- proc.time()
      fit.xgb.freq.tr <- xgb.train(data = xgb_vtrain.data.freq.tr,objective='count:poisson',nrounds = hyper_grid$n.trees[c],
                                   max_depth = hyper_grid$interaction.depth[c],eta = hyper_grid$shrinkage[c],colsample_by_tree = 0.8,subsample = 0.9,min_child_weight = 10,
                                   tree_method = "hist")
        
        
      (proc.time()-t1)}

      train_pred<- predict(fit.xgb.freq.tr, newdata= xgb_vtrain.data.freq.tr)
      internal.errors.tr[k] = dev_poiss(vtrain.data.freq.tr$ClaimNb,train_pred,vtrain.data.freq.tr$exposure )
        

      valid_pred=predict(fit.xgb.freq.tr, newdata =xgb_valid.data.freq.tr)
      pdev.errors.tr[k] = dev_poiss(valid.data.freq.tr$ClaimNb,valid_pred,valid.data.freq.tr$exposure)


          }

          int.output.tr[,c]=internal.errors.tr
          pred.output.tr[,c]=pdev.errors.tr

          err.out.freq[c] = mean(pdev.errors.tr) # mean CV error
        }
        err.cv.out[,t]=err.out.freq
        cp.opt[t,b] = which.min(err.out.freq)
    
        #####  The external loop model coded here 

  xgb_vtrain.data.freq <- xgb.DMatrix(data = data.matrix(vtrain.data.freq [,features.xgb.freq]), label = vtrain.data.freq$ClaimNb)
  xgb_valid.data.freq <- xgb.DMatrix(data = data.matrix(valid.data.freq[,features.xgb.freq]), label = valid.data.freq$ClaimNb)
  
  setinfo(xgb_vtrain.data.freq,"base_margin",log(vtrain.data.freq$exposure))
  setinfo(xgb_valid.data.freq,"base_margin",log(valid.data.freq$exposure))
  
  # train model
  {t1 <- proc.time()
    xgb_freq_cp_opt<- xgb.train(data = xgb_vtrain.data.freq,objective='count:poisson',nrounds = hyper_grid$n.trees[cp.opt[t,b]],
                                max_depth =hyper_grid$interaction.depth[cp.opt[t,b]] ,eta = hyper_grid$shrinkage[cp.opt[t,b]],colsample_by_tree = 0.8,subsample = 0.9,min_child_weight = 10,
                                tree_method = "hist")
    
      
    (proc.time()-t1)}
  
########################         
    y.internal <- predict(xgb_freq_cp_opt, newdata = xgb_vtrain.data.freq, type="response")
    err.internal[t,b] =dev_poiss(vtrain.data.freq$ClaimNb, y.internal,vtrain.data.freq$exposure)
    
    y.pred <- predict(xgb_freq_cp_opt, newdata = xgb_valid.data.freq, type="response")
    err.kfold[t,b] =dev_poiss(valid.data.freq$ClaimNb,y.pred,valid.data.freq$exposure )
    
  }

 print(b)
 }

####################### The cross validation loop ends here 
getmode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}

final.cp.opt= getmode(cp.opt)
#########   Fitting the GLM model to the train data 
xgb_train_data <- xgb.DMatrix(data = data.matrix(train_data [,features.xgb.freq]), label = train_data$ClaimNb)
xgb_test_data <- xgb.DMatrix(data = data.matrix(test_data[,features.xgb.freq]), label = test_data$ClaimNb)

setinfo(xgb_train_data,"base_margin",log(train_data$exposure))
setinfo(xgb_test_data,"base_margin",log(test_data$exposure))

# train model
xgb_freq_final<- xgb.train(data = xgb_train_data,objective='count:poisson',nrounds =hyper_grid$n.trees[final.cp.opt],
                               max_depth = hyper_grid$interaction.depth[final.cp.opt],eta = hyper_grid$shrinkage[final.cp.opt],
                               colsample_by_tree = 0.8,subsample = 0.9,min_child_weight = 10, tree_method = "hist")
  
xgb_freq_final$feature_names
var.names=colnames(train_data[features.xgb.freq])
imp_freq_final=xgb.importance(var.names,xgb_freq_final)

## frequency testing
Predicted_freq_train <-predict(xgb_freq_final, newdata = xgb_train_data, type="response")
Predicted_freq_train.error =dev_poiss(train_data$ClaimNb,Predicted_freq_train,train_data$exposure )

Predicted_freq_test<- predict(xgb_freq_final, newdata = xgb_test_data, type="response")
GLM.out.freq.err = dev_poiss(test_data$ClaimNb,Predicted_freq_test,test_data$exposure )


##################################################################################
######### Boot 632 for external error on correct data  ######
# ## Boot 632 of the external error
n_freq_correct=nrow(correct_data)
boot632_err_outfreq_correct=err_oob_outfreq_correct=inbag_err_freqout_correct=numeric(250)

for (t in 1:250) {
  ib = sample(1:n_freq_correct, size=1*n_freq_correct, replace=TRUE)
  inbag_sample_outfreq=correct_data[ib,]

  xgb_inbag_sample_outfreq <- xgb.DMatrix(data = data.matrix(inbag_sample_outfreq[,features.xgb.freq]), label = inbag_sample_outfreq$ClaimNb)
  pred_inbag_outfreq <- predict(xgb_freq_final, newdata=xgb_inbag_sample_outfreq, type="response")
  inbag_err_freqout_correct[t] =dev_poiss(inbag_sample_outfreq$ClaimNb,pred_inbag_outfreq,inbag_sample_outfreq$Exposure )


  #### oob error prediction

  oob_outfreq= which(!(correct_data$IDpol %in% inbag_sample_outfreq$IDpol))
  oob_sample_outfreq= correct_data[oob_outfreq,]

  xgb_oob_sample_outfreq <- xgb.DMatrix(data = data.matrix(oob_sample_outfreq[,features.xgb.freq]), label = oob_sample_outfreq$ClaimNb)
  pred_oob_outfreq <- predict(xgb_freq_final, newdata=xgb_oob_sample_outfreq,type="response")
  err_oob_outfreq_correct[t] =dev_poiss(oob_sample_outfreq$ClaimNb,pred_oob_outfreq,oob_sample_outfreq$Exposure )

  boot632_err_outfreq_correct[t] = 0.368 * err_oob_outfreq_correct[t] + 0.632 * inbag_err_freqout_correct[t]

}

bootstrap_freqout_correct=cbind(boot632_err_outfreq_correct,err_oob_outfreq_correct,inbag_err_freqout_correct)
write.csv(bootstrap_freqout_correct,"bootstrap_freqout_correct.csv")

####################### Severity Model, the data setup starts here 3########
train_data_nonzero =train_data[which(train_data$ClaimAmount!=0),] 
test_data_nonzero = test_data[which(test_data$ClaimAmount!=0),] 
train_data_nonzero$AverageClaim = as.numeric(train_data_nonzero$ClaimAmount/ train_data_nonzero$ClaimNb)
test_data_nonzero$AverageClaim = as.numeric(test_data_nonzero$ClaimAmount/ test_data_nonzero$ClaimNb)

######## The Cross Validation loop goes in here  
n = nrow(train_data_nonzero)
K = 5
folds = cut(1:n, K, labels=FALSE)
err.vinternal.sev=err.vout.sev= numeric(9)
err.cv.in.sev=err.cv.out.sev=matrix("NA",9,5)
pw.opt.sev=cp.opt.sev=err.internal.sev=err.kfold.sev =matrix(NA,5,1) 

p=10
set.seed(100)
for (b in 1:p) {
# ## re shuffling CV data
n_CV_sev = nrow(train_data_nonzero)
reord_CVsev = sample(1:n_CV_sev, size=n_CV_sev, replace=FALSE)
CV_data_sev = train_data_nonzero[reord_CVsev, ]

for(t in 1:K){
##training sample
i = which(folds==t)
vtrain.data.sev=CV_data_sev[-i,]
valid.data.sev=CV_data_sev[i,]

Ntr = nrow(vtrain.data.sev)
Ktr = 5
err.out.sev = numeric(9)
pw_cv_out.sev=int.output.tr.sev=pred.output.tr.sev=matrix(NA,5,9)

for (c in 1:9) {
# tuning tree model (equivalent to cv.glmnet())
tr.folds = cut(1:Ntr, Ktr, labels=FALSE)
internal.errors.tr.sev=pdev.errors.tr.sev = numeric(Ktr)
 
for(k in 1:Ktr){
ik = which(tr.folds==k)
vtrain.data.sev.tr = vtrain.data.sev[-ik,]
valid.data.sev.tr = vtrain.data.sev[ik,]

################### Extreme gradient boost matrix
features.xgb.sev <- setdiff( 1:length(vtrain.data.sev.tr) ,  grep("AverageClaim|id_policy|drv_sex1|drv_sex2|
                    vh_make|pol_bonus|pol_duration|pol_sit_duration|vh_make|drv_age2|drv_age_lic2|vh_din|vh_sale_begin|vh_sale_end|ClaimAmount|
                       |drv_age_avg|exposure|ClaimNb|drv_age_lic_avg|id_policy|vh_cyl", names(vtrain.data.sev.tr)  ))

xgb_vtrain.data.sev.tr <- xgb.DMatrix(data = data.matrix(vtrain.data.sev.tr [,features.xgb.sev]), label = vtrain.data.sev.tr$AverageClaim)
xgb_valid.data.sev.tr <- xgb.DMatrix(data = data.matrix(valid.data.sev.tr[,features.xgb.sev]), label = valid.data.sev.tr$AverageClaim)


#train model for the internal loop 
{t1 <- proc.time()
fit.xgb.sev.tr <- xgb.train(data = xgb_vtrain.data.sev.tr,objective='reg:gamma',nrounds = hyper_grid$n.trees[c],
                             max_depth = hyper_grid$interaction.depth[c],eta = hyper_grid$shrinkage[c],colsample_by_tree = 0.8,subsample = 0.9,min_child_weight = 10,
                            weight=vtrain.data.sev.tr$ClaimNb)


(proc.time()-t1)}

train_pred_sev<- predict(fit.xgb.sev.tr, newdata= xgb_vtrain.data.sev.tr)
internal.errors.tr.sev[k] = dev_gamma(vtrain.data.sev.tr$AverageClaim,train_pred_sev, vtrain.data.sev.tr$ClaimNb)

valid_pred_sev=predict(fit.xgb.sev.tr, newdata =xgb_valid.data.sev.tr)
pdev.errors.tr.sev[k] = dev_gamma(valid.data.sev.tr$AverageClaim,valid_pred_sev, valid.data.sev.tr$ClaimNb)
  
}
### Writing out the internal loop results

int.output.tr.sev[,c]=internal.errors.tr.sev
pred.output.tr.sev[,c]=pdev.errors.tr.sev

err.out.sev[c] = mean(pdev.errors.tr.sev) # mean CV error
}
err.cv.out.sev[,t]=err.out.sev
cp.opt.sev[t,b] = which.min(err.out.sev)

#####  The external loop model coded here 
xgb_vtrain.data.sev <- xgb.DMatrix(data = data.matrix(vtrain.data.sev [,features.xgb.sev]), label = vtrain.data.sev$AverageClaim)
xgb_valid.data.sev <- xgb.DMatrix(data = data.matrix(valid.data.sev[,features.xgb.sev]), label = valid.data.sev$AverageClaim)

# train model
{t1 <- proc.time()
  xgb_sev_cp_opt<- xgb.train(data = xgb_vtrain.data.sev,objective='reg:gamma',nrounds = hyper_grid$n.trees[cp.opt.sev[t,b]],
                              max_depth =hyper_grid$interaction.depth[cp.opt.sev[t,b]] ,eta = hyper_grid$shrinkage[cp.opt.sev[t,b]],colsample_by_tree = 0.8,subsample = 0.9,min_child_weight = 10,
                             weight=vtrain.data.sev$ClaimNb)
  
  
  (proc.time()-t1)}

########################         
y.internal.sev <- predict(xgb_sev_cp_opt, newdata = xgb_vtrain.data.sev, type="response")
err.internal.sev[t,b] =dev_gamma(vtrain.data.sev$AverageClaim, y.internal.sev,vtrain.data.sev$ClaimNb)

y.pred.sev <- predict(xgb_sev_cp_opt, newdata = xgb_valid.data.sev, type="response")
err.kfold.sev[t,b] =dev_gamma(valid.data.sev$AverageClaim,y.pred.sev,valid.data.sev$ClaimNb )

}
 print(b)
 }

####################### The cross validation loop ends here 
getmode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}
final.cp.opt.sev=getmode(cp.opt.sev)

#########   Fitting the GLM model to the train data 
xgb_train_data_sev <- xgb.DMatrix(data = data.matrix(train_data_nonzero [,features.xgb.sev]), label = train_data_nonzero$AverageClaim)
xgb_test_data_sev <- xgb.DMatrix(data = data.matrix(test_data_nonzero[,features.xgb.sev]), label = test_data_nonzero$AverageClaim)

# train model
xgb_sev_final<- xgb.train(data = xgb_train_data_sev,objective='reg:gamma',nrounds =hyper_grid$n.trees[final.cp.opt.sev],
                           max_depth = hyper_grid$interaction.depth[final.cp.opt.sev],eta = hyper_grid$shrinkage[final.cp.opt.sev],
                           colsample_by_tree = 0.8,subsample = 0.9,min_child_weight = 10, weight=train_data_nonzero$ClaimNb)

xgb_sev_final$feature_names
var.names=colnames(train_data[features.xgb.sev])
imp_sev_final=xgb.importance(var.names,xgb_sev_final)

## frequency testing
Predicted_sev_train <-predict(xgb_sev_final, newdata = xgb_train_data_sev, type="response")
Predicted_sev_train.error =dev_gamma(train_data_nonzero$AverageClaim,Predicted_sev_train,train_data_nonzero$ClaimNb)
Predicted_sev_test<- predict(xgb_sev_final, newdata = xgb_test_data_sev, type="response")
GLM.out.sev.err = dev_gamma(test_data_nonzero$AverageClaim,Predicted_sev_test,test_data_nonzero$ClaimNb)
###### Premiums to be used for APTP Analysis
# Predicted_freq_train <-predict(xgb_freq_final, newdata = xgb_train_data, type="response")
# Predicted_freq_test<- predict(xgb_freq_final, newdata = xgb_test_data, type="response")
# Predicted_sev_test_full= predict(xgb_sev_final, newdata = xgb_test_data, type="response")
# Predicted_sev_train_full= predict(xgb_sev_final, newdata =xgb_train_data, type="response")
# colnames(xgb_test_data)
# 
# Prem_train= Predicted_sev_train_full * Predicted_freq_train
# Prem_test= Predicted_sev_test_full * Predicted_freq_test
# train_data$Prem_train=Prem_train
# test_data$Prem_test=Prem_test
# 
# write.csv(train_data,"train_data_download.csv")
# write.csv(test_data,"test_data_download.csv")
# data_download= rbind(train_data,test_data)
# write.csv(Predicted_sev_correct,"Predicted_sev_correct.csv")
# final_sev=cbind(Predicted_sev_train.error,GLM.out.sev.err)
# write.csv(final_sev,"final_sev.csv")

###########################################################################################################################
######### Boot 632 for external error on correct data  ######
## Boot 632 of the external error
# n_sev_correct_nonzero=nrow(correct_data_nonzero)
# boot632_err_outsev_correct=err_oob_outsev_correct=inbag_err_sevout_correct=numeric(250)
# 
# for (t in 1:250) {
# ib_sev = sample(1:n_sev_correct_nonzero, size=1*n_sev_correct_nonzero, replace=TRUE)
# inbag_sample_outsev=correct_data_nonzero[ib_sev,]
# xgb_inbag_sample_outsev <- xgb.DMatrix(data = data.matrix(inbag_sample_outsev[,features.xgb.sev]), label = inbag_sample_outsev$AverageClaim)
# pred_inbag_outsev <- predict(xgb_sev_final, newdata=xgb_inbag_sample_outsev, type="response")
# inbag_err_sevout_correct[t] =dev_gamma(inbag_sample_outsev$AverageClaim,pred_inbag_outsev,inbag_sample_outsev$ClaimNb )
# 
# #### oob errror prediction
# oob_outsev= which(!(correct_data_nonzero$IDpol %in% inbag_sample_outsev$IDpol))
# oob_sample_outsev= correct_data_nonzero[oob_outsev,]
# xgb_oob_sample_outsev <- xgb.DMatrix(data = data.matrix(oob_sample_outsev[,features.xgb.sev]), label = oob_sample_outsev$AverageClaim)
# pred_oob_outsev <- predict(xgb_sev_final, newdata=xgb_oob_sample_outsev,type="response")
# err_oob_outsev_correct[t] =dev_gamma(oob_sample_outsev$AverageClaim,pred_oob_outsev,oob_sample_outsev$ClaimNb )
# boot632_err_outsev_correct[t] = 0.368 * err_oob_outsev_correct[t] + 0.632 * inbag_err_sevout_correct[t]
# }
# 
# bootstrap_sevout_correct=cbind(boot632_err_outsev_correct,err_oob_outsev_correct,inbag_err_sevout_correct)
# write.csv(bootstrap_sevout_correct,"bootstrap_sevout_correct.csv")

################################################################################
###### Bias calculation 

xgb_full_data <- xgb.DMatrix(data = data.matrix(dat_freq [,features.xgb.freq]), label = dat_freq$ClaimAmount)
Predicted_freq_correct<- predict(xgb_freq_final, newdata = xgb_full_data, type="response")
Predicted_sev_correct_withzero= predict(xgb_sev_final, newdata=xgb_full_data, type="response")
Predicted.premium_correct= Predicted_freq_correct*Predicted_sev_correct_withzero
AP_Output= cbind(Predicted.premium_correct,dat_freq$IDpol)
write.csv(AP_Output,"AP_Output.csv")
bias_correct_data=sum(Predicted.premium_correct)-sum(correct_data$ClaimAmount)
bias_percentage_correct_data=bias_correct_data/sum(correct_data$ClaimAmount)


