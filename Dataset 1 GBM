   #install.packages("gbm")
  # install.packages("pdp")
  # install.packages("vip")
  #install.packages("lime")
  # install.packages("ggplot2")
  # install.packages("tibble")
  # install.packages("scales")
  # install.packages("rlang", type = "source")
  # install.packages("glue")
  library(Metrics)
  library(ISLR)
  library(h2o)
  require(MASS)
  library(CASdatasets)
  require(stats)
  library(data.table)
  library(plyr)
  library(rpart)
  library(rpart.plot)
  library(Hmisc)
  library(glmnet)
  library(dplyr)
  library(gbm)
  library(pdp)           # for partial dependence plots
  library(vip)           # for variable importance plots
  library(ggplot2)
  library(lime)         # model visualization
  library(tibble)
  library(dplyr)
  library(h2o)
  # remotes::update_packages("rlang")
  # remotes::update_packages("tidyverse")
  #### Library for Severity model 
  # install.packages("devtools")
  # devtools::install_github('henckr/distRforest')
  library(distRforest)
  data(pg17trainpol)
  data(pg17trainclaim)
  data(pg17testyear1)
  data(pg17testyear2)
  data(pg17testyear3)
  data(pg17testyear4)
  data(fredpt17)
  library(ggcorrplot)
  library(ggplot2)
  library(locfit)
  library(Metrics)
  library(ISLR)
  library(h2o)
  require(MASS)
  library(CASdatasets)
  require(stats)
  library(data.table)
  library(plyr)
  library(rpart)
  library(rpart.plot)
  library(Hmisc)
  library(glmnet)
  library(dplyr)
  data(freMTPL2freq)
  data(freMTPL2sev)
  library("wesanderson")
  library(statmod)
  library(scales)
  ##################################################
  clrpal = wes_palette("Zissou1", 22, type = "continuous")
  clrpallow = scales::alpha(clrpal,.4)
  bleurouge = clrpal[c(1,22)]
  bleurougepal = clrpallow[c(1,22)]
  colr= clrpal4 = wes_palette("Darjeeling1")[c(1,2,3,5)]
  clrpal6 = wes_palette("Zissou1", 6, type = "continuous")
  ### Initial data call 
  #### Aggreagating the two datasets together
  freMTPL2sev_tot = aggregate(freMTPL2sev$ClaimAmount,
                              by=list(freMTPL2sev$IDpol),
                              FUN = sum)
  names(freMTPL2sev_tot)=names(freMTPL2sev)
  freMTPL2 = merge(freMTPL2freq,freMTPL2sev_tot,all.x=TRUE)
  freMTPL2$ClaimAmount[is.na(freMTPL2$ClaimAmount)]=0
  
  #### Limiting to only high Exposure policies 
  freMTPL2 = freMTPL2[(freMTPL2$Exposure>.8)&(freMTPL2$Exposure<=1),]
  ##### Removing all large claim values 
  freMTPL2 = freMTPL2[freMTPL2$ClaimAmount<=10000,]
  dat_freq=freMTPL2  
  dat_freq_nonzero = dat_freq[dat_freq$ClaimAmount!=0,]
  dat_freq_zero = dat_freq[dat_freq$ClaimAmount==0,]
  n_dat_freq_zero=nrow(dat_freq_zero)
  set.seed(1001)
  keep_in= sample(1:n_dat_freq_zero, size=.4*n_dat_freq_zero, replace=FALSE)
  dat_freq_zero_keep= dat_freq_zero[keep_in,]  
  dat_freq= rbind(dat_freq_nonzero,dat_freq_zero_keep)  
  #########################################################################################  
  ######## Replacing the FreqMTPL2 label to dat_freq that we commonly use in other code
  ######## Note dat_freq has both frequemcy and severity data 
  freMTPL2_backup = freMTPL2
  ####### Making adjustments to data 
  dat_freq$VehGas <- factor(dat_freq$VehGas)    # consider VehGas as categorical
  dat_freq$ClaimNb <- pmin(dat_freq$ClaimNb, 4)   # correct for unreasonable observations (that might be data error)
  dat_freq$Exposure <- pmin(dat_freq$Exposure, 1) # correct for unreasonable observations (that might be data error)
  dat_freq$Area <- as.factor(dat_freq$Area)   ### Area as factor 
  dat_freq$VehPower <- as.factor(pmin(dat_freq$VehPower,9))
  dat_freq$BonusMalus <- as.integer(pmin(dat_freq$BonusMalus, 150))
  dat_freq$Density <- as.numeric(log(dat_freq$Density))
  
  ############## Removing Severity data 
  dat_freq_temp= dat_freq[dat_freq$ClaimAmount!=0,]
  ID_Exclude1 = dat_freq_temp$IDpol[which(dat_freq_temp$ClaimAmount<100)]
  ID_Exclude2 = dat_freq_temp$IDpol[which(dat_freq_temp$ClaimAmount %in% c(1204, 1128.12,1172,1128))]
  dat_freq= dat_freq[-which(dat_freq$IDpol %in% c(ID_Exclude1,ID_Exclude2)),]
  ##### Shuffling the data
  n_freq = nrow(dat_freq)
  set.seed(100)
  reord_freq = sample(1:n_freq, size=n_freq, replace=FALSE)
  dat_freq = dat_freq[reord_freq, ] 
  dat_freq$ClaimNb=as.numeric(dat_freq$ClaimNb)
  
  features.freq <- setdiff( 1:length(dat_freq) ,  grep("drv_age_lic_avg|id_policy|drv_sex2|vh_make|
      vh_make|drv_age2|drv_age_lic2|vh_din|vh_sale_begin|vh_sale_end|ClaimAmount|drv_age_avg|Exposure|ClaimNb|
      drv_age_lic_avg|vh_cyl", names(dat_freq)))
  
  set.seed(1001)
  split1= sample(1:n_freq, size=.833*n_freq, replace=FALSE)
  CV_data_freq = dat_freq[split1,]
  test_data_freq = dat_freq[-split1,]
  
  dev_poiss <- function(ytrue, ypred, wcase) -2 * weighted.mean(dpois(ytrue, ypred, log = TRUE) - dpois(ytrue, ytrue, log = TRUE), wcase, na.rm = TRUE)
  dev_gamma <- function(ytrue, ypred, wcase) -2 * weighted.mean(log(ytrue/ypred) - ((ytrue - ypred) / ypred), wcase, na.rm = TRUE)
  
##### GBM Model fitting
    hyper_grid <- expand.grid(
    n.trees = c(2000),
    interaction.depth = c(1,2,3)
  )
  
############## CV for frequency ############################
  K=5
  n = nrow(CV_data_freq)
  folds = cut(1:n, K, labels=FALSE)
  err.vinternal.freq=err.vout.freq= numeric(3)
  err.cv.in=err.cv.out=matrix("NA",3,5)
  cp.opt=numeric(5)
  err.cv.in.fin= err.cv.out.fin=matrix(NA,5,10)
  
  p=10
  
for (t in 1:p) {

  ## re shuffling CV data
  n_CV_freq = nrow(CV_data_freq)
    reord_CVfreq = sample(1:n_CV_freq, size=n_CV_freq, replace=FALSE)
    CV_data_freq = CV_data_freq[reord_CVfreq, ]
    
  
for (a in 1:K) {
   
     i = which(folds==a)
    vtrain.data.freq=CV_data_freq[-i,]
    valid.data.freq=CV_data_freq[i,]
    Ntr = nrow(vtrain.data.freq)
    Ktr = 5
    err.out.freq = numeric(3)
    int.output.tr=pred.output.tr=matrix(NA,5,3)
    
################ Hyper Parameter training Starts 
for (c in 1:3) {# tuning tree model (equivalent to cv.glmnet())

    tr.folds = cut(1:Ntr, Ktr, labels=FALSE)
    internal.errors.tr= pdev.errors.tr = numeric(Ktr)
for(k in 1:Ktr){

      ik = which(tr.folds==k)
      vtrain.data.freq.tr = vtrain.data.freq[-ik,]
      valid.data.freq.tr = vtrain.data.freq[ik,]

      gbm_freq_valid.tr <- gbm( formula = ClaimNb ~offset(log(vtrain.data.freq.tr$Exposure))+ Area + VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Density + Region, distribution = "poisson",
        data = vtrain.data.freq.tr, shrinkage = 0.01 ,n.trees = hyper_grid$n.trees[c], interaction.depth = hyper_grid$interaction.depth[c],n.cores = NULL, verbose = FALSE)


      train_pred<-exp(predict(gbm_freq_valid.tr, newdata= vtrain.data.freq.tr))*vtrain.data.freq.tr$Exposure
      internal.errors.tr[k] = dev_poiss(vtrain.data.freq.tr$ClaimNb,train_pred,vtrain.data.freq.tr$Exposure)


    valid_pred <- exp(predict(gbm_freq_valid.tr, newdata=valid.data.freq.tr))*valid.data.freq.tr$Exposure
    pdev.errors.tr[k] = dev_poiss(valid.data.freq.tr$ClaimNb,valid_pred,valid.data.freq.tr$Exposure)


}
    ### Writing out the internal loop results

    int.output.tr[,c]=internal.errors.tr
    pred.output.tr[,c]=pdev.errors.tr

  err.out.freq[c] = mean(pdev.errors.tr) # mean CV error
}

  write.csv(int.output.tr,paste0(a,"int.output.tr.csv"))
  write.csv(pred.output.tr,paste0(a,"pred.output.tr.csv"))

err.cv.out[,a]=err.out.freq
cp.opt[a] = which.min(err.out.freq)
##################### Hyper Parameter training Ends here
  ## Training the CV model 
  
  gbm_freq_cp_opt <- gbm(
    formula = ClaimNb ~ offset(log(vtrain.data.freq$Exposure))+ Area + VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Density + Region , distribution = "poisson",
    data = vtrain.data.freq, shrinkage = 0.01 ,n.trees =hyper_grid$n.trees[cp.opt[a]] , interaction.depth = hyper_grid$interaction.depth[cp.opt[a]],n.cores = NULL, verbose = FALSE)

    valid_pred_fin <- exp(predict(gbm_freq_cp_opt, newdata=valid.data.freq))*valid.data.freq$Exposure
  err.cv.out.fin[a,t]=dev_poiss(valid.data.freq$ClaimNb,valid_pred_fin,valid.data.freq$Exposure)
  
  internal_pred_fin <- exp(predict(gbm_freq_cp_opt, newdata=vtrain.data.freq))*vtrain.data.freq$Exposure
  err.cv.in.fin[a,t]=dev_poiss(vtrain.data.freq$ClaimNb,internal_pred_fin,vtrain.data.freq$Exposure)
  
  }
}

################# CV for frequency ends #####################
  write.csv(err.cv.out.fin,"err.cv.out.fin.csv")
  write.csv(err.cv.in.fin,"err.cv.in.fin.csv")
  
  # (1) Prediction performance from CV:
  mean(err.cv.out.fin)
  
  # (2) Independent test error evaluation:
  final.cp.opt.test=apply(err.cv.out,2,which.min)
  final.cp.opt=round(mean(final.cp.opt.test))
  #final.cp.opt = which(err.cv.out == min(err.cv.out), arr.ind = TRUE)[1]

  gbm_freq <- gbm(
    formula = ClaimNb ~ offset(log(CV_data_freq$Exposure))+ Area + VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Density + Region, distribution = "poisson",
    data = CV_data_freq, shrinkage = 0.01 ,n.trees =hyper_grid$n.trees[final.cp.opt] , interaction.depth = hyper_grid$interaction.depth[final.cp.opt],n.cores = NULL, verbose = FALSE)

  importance_freq=summary(gbm_freq)
  write.csv(importance_freq,"importance_freq.csv")

##### Learn and test Errors/ Deviance 
  # importance(gbm_freq)
  
  pred_internal_freq= exp(predict(gbm_freq, newdata =  CV_data_freq,n.trees = 2000))*CV_data_freq$Exposure
  pred_external_freq=exp(predict(gbm_freq,test_data_freq,n.trees = 2000))*test_data_freq$Exposure
  write.csv(pred_external_freq,"pred_external_freq.csv")
  write.csv(test_data_freq,"test_data_freq.csv")
    
  err.internal.fin.freq=dev_poiss(CV_data_freq$ClaimNb,pred_internal_freq,CV_data_freq$Exposure)
  err.out.fin.freq=dev_poiss(test_data_freq$ClaimNb,pred_external_freq,test_data_freq$Exposure)  
  
  final_err_freq=cbind(err.internal.fin.freq,err.out.fin.freq)
  write.csv(final_err_freq,"final_err_freq.csv")
  
  ######## Boot 632 for external error ######
  ## Boot 632 of the external error
  
  n_freq=nrow(test_data_freq)
  boot632_err_outfreq=err_oob_outfreq=inbag_err_freqout=numeric(250)
  
  for (t in 1:250) {
    ib = sample(1:n_freq, size=1*n_freq, replace=TRUE)
    inbag_sample_outfreq=test_data_freq[ib,]
    
    pred_inbag_outfreq <- exp(predict(gbm_freq, newdata=inbag_sample_outfreq))*inbag_sample_outfreq$Exposure
    inbag_err_freqout[t] =dev_poiss(inbag_sample_outfreq$ClaimNb,pred_inbag_outfreq,inbag_sample_outfreq$Exposure)
    
    #### oob errror prediction
    
    oob_outfreq= which(!(test_data_freq$IDpol %in% inbag_sample_outfreq$IDpol))     
    oob_sample_outfreq= test_data_freq[oob_outfreq,]
    
    pred_oob_outfreq <- exp(predict(gbm_freq, newdata=oob_sample_outfreq))*oob_sample_outfreq$Exposure
    err_oob_outfreq[t] =dev_poiss(oob_sample_outfreq$ClaimNb,pred_oob_outfreq,oob_sample_outfreq$Exposure)
    
    boot632_err_outfreq [t] = 0.368 * err_oob_outfreq[t] + 0.632 * inbag_err_freqout[t]
    
  }
  
  bootstrap_freqout=cbind(boot632_err_outfreq,err_oob_outfreq,inbag_err_freqout)
  write.csv(bootstrap_freqout,"bootstrap_freqout.csv")
  range_freqout= cbind(quantile(boot632_err_outfreq,0.025),quantile(boot632_err_outfreq,0.975))
  
######################################################################################
###### Severity Model
#### data processing: 
dat_severity=dat_freq[dat_freq$ClaimAmount!=0,]
n_severity = nrow(dat_severity)
set.seed(1001)
reord_sev = sample(1:n_severity, size=n_severity, replace=FALSE)
dat_severity = dat_severity[reord_sev, ] 
dat_severity$Average_claim=dat_severity$ClaimAmount/dat_severity$ClaimNb
# save a validation sample for later:
set.seed(100)
i.valid_sev = sample(1:n_severity, size=.833*n_severity, replace=FALSE)
CV_data_sev = dat_severity[i.valid_sev,]
test_data_sev = dat_severity[-i.valid_sev,]
predictors <- c("VehPower","VehAge","DrivAge","BonusMalus","VehBrand","VehGas","Area","Density","Region")
response <- "Average_claim"
claimNbr= "ClaimNb"
CV_data_sev=as.data.frame(CV_data_sev)

# CV_data_sev$IDpol= as.numeric(CV_data_sev$IDpol)
# CV_data_sev$ClaimNb=as.numeric(CV_data_sev$ClaimNb)
# CV_data_sev$Exposure=as.numeric(CV_data_sev$Exposure)
# CV_data_sev$Sum_amount=as.numeric(CV_data_sev$Sum_amount)
# CV_data_sev$Average_claim= as.numeric(CV_data_sev$Average_claim)

############## CV for Severity
K=5
n_sev = nrow(CV_data_sev)
folds_sev = cut(1:n_sev, K, labels=FALSE)
err.out.sev=err.vinternal.sev=err.vout.sev= numeric(3)
err.cvin.sev=err.cvout.sev=matrix("NA",3,5)
cp.opt.sev=numeric(5)
err.cvin.fin.sev=err.cvout.fin.sev=matrix(NA,5,10)
p=10

 for (n in 1:p) {

  ## re shufling CV data
  n_CV_sev = nrow(CV_data_sev)
  reord_CVsev = sample(1:n_CV_sev, size=n_CV_sev, replace=FALSE)
  CV_data_sev = CV_data_sev[reord_CVsev,]

 for (a in 1:K) {
  i = which(folds_sev==a)
  vtrain.data.sev=CV_data_sev[-i,]
  valid.data.sev=CV_data_sev[i,]
  Ntr = nrow(vtrain.data.sev)
  Ktr = 5
###################################### Code for Hyperparameter tunning starts here ##########
for (c in 1:3) {
     tr.folds = cut(1:Ntr, Ktr, labels=FALSE)
    pdev.errors.tr = numeric(Ktr)
for(k in 1:Ktr){
      ik = which(tr.folds==k)
      vtrain.data.sev.tr = vtrain.data.sev[-ik,]
      valid.data.sev.tr = vtrain.data.sev[ik,]
      h2o.init(max_mem_size = "20G")    ## specify the memory size for the H2O cloud
      h2o_vtrain.data.sev.tr=as.h2o(vtrain.data.sev.tr)
      h2o_valid.data.sev.tr =as.h2o(valid.data.sev.tr)

      gbm_CV_sev.tr <- h2o.gbm(x= predictors,y=response, weights_column = claimNbr,learn_rate=0.001,
                               training_frame = h2o_vtrain.data.sev.tr, ntrees =hyper_grid$n.trees[c],max_depth =hyper_grid$interaction.depth[c], distribution = "gamma",min_rows=10 )

      valid_pred <- as.vector(predict(gbm_CV_sev.tr, newdata=h2o_valid.data.sev.tr))*valid.data.sev.tr$ClaimNb

      pdev.errors.tr[k] = dev_gamma(valid.data.sev.tr$Average_claim,valid_pred, valid.data.sev.tr$ClaimNb)
    }
    err.out.sev[c] = mean(pdev.errors.tr) # mean CV error
  }
err.cvout.sev[,a]=err.out.sev
cp.opt.sev[a] =which.min(err.out.sev)
###################################### Code for Hyperparameter tunning ends here ##########

  # h2o.init(max_mem_size = "20G")    ## specify the memory size for the H2O cloud
  h2o_vtrain.data.sev=as.h2o(vtrain.data.sev)
  h2o_valid.data.sev =as.h2o(valid.data.sev)
  
    gbm_cp_out_sev <-h2o.gbm(x= predictors,y=response, weights_column = claimNbr,learn_rate=0.001, 
                             training_frame = h2o_vtrain.data.sev, ntrees =hyper_grid$n.trees[cp.opt.sev[a]],max_depth =hyper_grid$interaction.depth[cp.opt.sev[a]], distribution = "gamma",min_rows=10 )
    
  valid_pred_fin.sev <- as.vector(predict(gbm_cp_out_sev, newdata=h2o_valid.data.sev))
  err.cvout.fin.sev[a,n]=dev_gamma(valid.data.sev$Average_claim,valid_pred_fin.sev,valid.data.sev$ClaimNb)
  
  internal_pred_fin.sev <- as.vector(predict(gbm_cp_out_sev, newdata=h2o_vtrain.data.sev))
  err.cvin.fin.sev[a,n]=dev_gamma(vtrain.data.sev$Average_claim,internal_pred_fin.sev, vtrain.data.sev$ClaimNb)
  
}
}

write.csv(err.cvout.sev,"Tunning Severity Error.csv")
write.csv(err.cvout.fin.sev,"err.cvout.fin.sev.csv")
write.csv(err.cvin.fin.sev,"err.cvin.fin.sev.csv")

################## End of CV for Severity
write.csv(cp.opt.sev,"cp.opt.sev.csv")

#Independent test error evaluation:
  final.cp.opt.test.sev=apply(err.cvout.sev,2,which.min)
  final.cp.opt.sev=round(mean(final.cp.opt.test.sev))
##
  test_data_sev=as.data.frame(test_data_sev)
  # test_data_sev$IDpol=as.numeric(test_data_sev$IDpol)
  # test_data_sev$ClaimNb=as.numeric(test_data_sev$ClaimNb)
  # test_data_sev$Exposure=as.numeric(test_data_sev$Exposure)
  # test_data_sev$Sum_amount=as.numeric(test_data_sev$Sum_amount)
  # test_data_sev$Average_claim=as.numeric(test_data_sev$Average_claim)

  h2o_CV_data_sev=as.h2o(CV_data_sev)
  h2o_test_data_sev=as.h2o(test_data_sev)


  gbm.severity <- h2o.gbm(x= predictors,y=response, weights_column = claimNbr,learn_rate=0.001, 
           training_frame = h2o_CV_data_sev, ntrees =hyper_grid$n.trees[final.cp.opt.sev],max_depth =hyper_grid$interaction.depth[final.cp.opt.sev], distribution = "gamma",min_rows=10)
           
##### Looking at our model
importance_severity=summary(gbm.severity)
write.csv(importance_severity,"importance_severity.csv")  

  pred_internal_sev= as.vector(predict(gbm.severity, newdata= h2o_CV_data_sev))
  pred_external_sev=as.vector(predict(gbm.severity, newdata= h2o_test_data_sev))
  
  err.fin.internal.sev=dev_gamma(CV_data_sev$Average_claim,pred_internal_sev,CV_data_sev$ClaimNb)

  err.fin.out.sev=dev_gamma(test_data_sev$Average_claim,pred_external_sev, test_data_sev$ClaimNb)

  ##### Bootstrap 632 for severity #######
  
  n_sev=nrow(test_data_sev)
  boot632_err_outsev=err_oob_outsev=inbag_err_sevout=numeric(250)
  
  for (t in 1:250) {
    ib_sev = sample(1:n_sev, size=1*n_sev, replace=TRUE)
    inbag_sample_outsev=test_data_sev[ib_sev,]
    h2o_inbag_sample_outsev=as.h2o(inbag_sample_outsev)
    
    pred_inbag_outsev <-as.vector(predict(gbm.severity, newdata=h2o_inbag_sample_outsev))
    inbag_err_sevout[t] =dev_gamma(inbag_sample_outsev$Average_claim,pred_inbag_outsev,inbag_sample_outsev$ClaimNb)
    
    #### oob errror prediction
    
    oob_outsev= which(!(test_data_sev$IDpol %in% inbag_sample_outsev$IDpol))     
    oob_sample_outsev= test_data_sev[oob_outsev,]
    h2o_oob_sample_outsev=as.h2o(oob_sample_outsev)
    
    pred_oob_outsev <- as.vector(predict(gbm.severity, newdata=h2o_oob_sample_outsev))
    err_oob_outsev[t] =dev_gamma(oob_sample_outsev$Average_claim,pred_oob_outsev,oob_sample_outsev$ClaimNb)
    
    boot632_err_outsev[t] = 0.368 * err_oob_outsev[t] + 0.632 * inbag_err_sevout[t]
  }
  
  bootstrap_sevout=cbind(boot632_err_outsev,err_oob_outsev,inbag_err_sevout)
  write.csv(bootstrap_sevout,"bootstrap_sevout.csv")
  range_sevout= cbind(quantile(boot632_err_outsev,0.025),quantile(boot632_err_outsev,0.975)) 
  
  ##### Complete premium values to check how the model performs overall
####### Taking the original Frequency test data to get the final prediction of premium#################
test_data_freq
#y.pred.freq <- predict(lasso_best.freq, s = best_lam, newx = x.test.freq, newoffset= log(x.test.expo), type="response")
  
### Convert test data freq as H2o
  ### test data conversion
  # test_data_freq$IDpol=as.numeric(test_data_freq$IDpol)
  # test_data_freq$ClaimNb=as.numeric(test_data_freq$ClaimNb)
  # test_data_freq$Exposure=as.numeric(test_data_freq$Exposure)
# h2o.shutdown(prompt = TRUE)
# h2o.init(min_mem_size = "20G")
  
h2o_test_data_freq= as.h2o(test_data_freq)

pred_external_freq   ### Predicted claims frequecny 

Pred_severity <- as.vector(predict(gbm.severity, newdata=h2o_test_data_freq)) #### Predicted Severity Final 

orignal_data= test_data_freq$ClaimAmount

a=sum(orignal_data)
predicted_premium= pred_external_freq*Pred_severity*test_data_freq$Exposure
b=sum(predicted_premium)

error_prem= (a-b)/a 

write.csv(predicted_premium,"predicted_prem_gbm")
write.csv(test_data_freq, "test_data_GBM")
# write.csv(orignal_data,"orignal_data_gbm")

####### Bootstrap 632 for bias ##########################################################
n = nrow(test_data_freq)
B = 250
pred_inbag=Actual_inbag=bias_inbag=percent.bias_inbag=pred_oob= Actual_oob= bias_oob= percent.bias_oob=boot632_bias= boot632_bias_perc=numeric(B)

 for(b in 1:B){
  ib_bias = sample(1:n, size=1*n, replace=TRUE)
  
  ####### in sample bias calculation
  inbag_sample_bias=test_data_freq[ib_bias,]
  inbag_sample_bias$IDpol=as.numeric(inbag_sample_bias$IDpol)
  inbag_sample_bias$ClaimNb=as.numeric(inbag_sample_bias$ClaimNb)
  inbag_sample_bias$Exposure= as.numeric(inbag_sample_bias$Exposure)
  h2o_inbag_sample_bias= as.h2o(inbag_sample_bias)
  
  
  #### Freq for bias################################################################
  pred_freq_inbag <- exp(predict(gbm_freq, newdata=inbag_sample_bias))*inbag_sample_bias$Exposure
  
  ### Severity part of the bias
  Pred_severity_inbag <- as.vector(predict(gbm.severity, newdata=h2o_inbag_sample_bias))#### Predicted Severity Final
  ### comparison step
  orignal_data_inbag= inbag_sample_bias$ClaimAmount

  predicted_premium_inbag= pred_freq_inbag*Pred_severity_inbag*inbag_sample_bias$Exposure
  
  pred_inbag[b]=sum(predicted_premium_inbag)
  Actual_inbag[b]= sum(orignal_data_inbag)
  
  bias_inbag[b] = Actual_inbag[b]-pred_inbag[b]
  percent.bias_inbag[b]=bias_inbag[b]/ Actual_inbag[b]
  
  ##### Out of bag bias evaluation ############################################ 
  
  oob_sample= which(!(test_data_freq$IDpol %in% inbag_sample_bias$IDpol))
  oob_sample_bias= test_data_freq[oob_sample,]
  oob_sample_bias$IDpol=as.numeric(oob_sample_bias$IDpol)
  oob_sample_bias$ClaimNb=as.numeric(oob_sample_bias$ClaimNb)
  oob_sample_bias$Exposure= as.numeric(oob_sample_bias$Exposure)
  h2o_oob_sample_bias= as.h2o(oob_sample_bias)
  
  #### Freq for bias###################################
  
  pred_freq_oob <- exp(predict(gbm_freq, newdata=oob_sample_bias))*oob_sample_bias$Exposure
  ### Severity part of the bias
  Pred_severity_oob <-as.vector(predict(gbm.severity, newdata=h2o_oob_sample_bias)) #### Predicted Severity Final
  ### comparison step
  orignal_data_oob= oob_sample_bias$ClaimAmount
    
  predicted_premium_oob= pred_freq_oob*Pred_severity_oob*oob_sample_bias$Exposure
  
  pred_oob[b]=sum(predicted_premium_oob)
  Actual_oob[b]= sum(orignal_data_oob)
  
  bias_oob[b] = Actual_oob[b]-pred_oob[b]
  percent.bias_oob[b]=bias_oob[b]/ Actual_oob[b]
  
  ########### Final 632 errors to be used #####################################
  
  boot632_bias[b] = 0.368 * bias_oob[b] + 0.632 * bias_inbag[b]
  
  boot632_bias_perc[b] = 0.368 * percent.bias_oob[b] + 0.632 * percent.bias_inbag[b]
  
}

bootstrap_bias=cbind(bias_inbag,percent.bias_inbag,bias_oob,percent.bias_oob,boot632_bias, boot632_bias_perc)
write.csv(bootstrap_bias,"bootstrap_bias.csv")
boxplot(boot632_bias_perc)

######## Bias Calculation 
  # 
# n = nrow(test_data_freq)
# 
# h2o.init()
# B = 100
# bias=Actual.bias=pred.bias= percent.bias=numeric(B)
# for(b in 1:B){
#   ib = sample(1:n, size=1*n, replace=TRUE)
#   test_data_freq_bias= test_data_freq[ib,]
#   
#   #### 
#   test_data_freq_bias$IDpol=as.numeric(test_data_freq_bias$IDpol)
#   test_data_freq_bias$ClaimNb=as.numeric(test_data_freq_bias$ClaimNb)
#   test_data_freq_bias$Exposure=as.numeric(test_data_freq_bias$Exposure)
#   h2o_test_data_freq_bias= as.h2o(test_data_freq_bias)
#   
#   
#   #### Freq for bias 
#   pred.bias.freq <- predict(gbm_freq, newdata=test_data_freq_bias)
#   pred.bias.freq=exp(pred.bias.freq)*test_data_freq_bias$Exposure
#   
#   ### Sevrity part of the bias
#   Pred_severity_bias <- as.vector(predict(gbm.severity, newdata=h2o_test_data_freq_bias)) #### Predicted Severity Final 
#   # Pred_severity_bias=exp(Pred_severity_bias)
#   
#   
#   ### comparison step
#   orignal_data_bias= merge(test_data_freq_bias,dat_sev2,by="IDpol")
#   
#   predicted_premium_bias= pred.bias.freq*Pred_severity_bias*test_data_freq_bias$Exposure
#   
#   pred.bias[b]=sum(predicted_premium_bias)
#   
#   Actual.bias[b]= sum(orignal_data_bias$Sum_amount)
#   
#   bias[b] = Actual.bias[b]-pred.bias[b]
#   
#   percent.bias[b]=bias[b]/ Actual.bias[b]
#   
# }
# 
# ##### Output
# err.internal.freq
# err.out.freq
# 
# err.internal_sev
# err.internal2_sev
# err.out_sev
# err.out2_sev
# 
# sum(orignal_data$Sum_amount)
# sum(predicted_premium)


# out=cbind(pred.bias,Actual.bias,bias, percent.bias)
# View(out)
# write.csv(out,"out.csv")
# 
# percent.bias
# mean(bias)
# sd(bias)

######################################### Further development for later

# # plot loss function as a result of n trees added to the ensemble
# X11()
# gbm.perf(gbm.fit, method = "cv")
# gbm.perf(gbm.severity, method = "cv")
# 
# #######Variable imporatance
# x11()
# summary(gbm.fit)
# summary(gbm.severity)
#
# ##### Interaction effect # Friedman's H-statistic
# # interact.gbm(gbm.fit, train_data_freq, i.var = "Area", n.trees = gbm.fit$n.trees)
# #rm(int.h)
# name_vec=colnames(train_data_sev)
# name_vec=name_vec[-c(1:3,13:14)]
# combns <- t(combn(name_vec, m = 2))
# int.h <- numeric(nrow(combns))
# 
# for (i in 1:nrow(combns)) {
#   # print(paste("iter", i, "of", nrow(combns)))
#   int.h[i] <- interact.gbm(gbm.severity, data = train_data_sev, i.var = combns[i, ], 
#                            n.trees = gbm.severity$n.trees)
# }
# int.h <- data.frame(x = paste0(combns[, 1L], "*", combns[, 2L]), y = int.h)
# int.h <- int.h[order(int.h$y, decreasing = TRUE), ]
# 
# pint <- ggplot(int.h[1:10, ], aes(reorder(x, y), y)) +
#   geom_col(width = 0.75) +
#   labs(x = "", y = "Interaction strength", title = "Friedman's H-statistic") +
#   theme(axis.text.x = element_text(angle = 45, vjust = 1)) +
#   theme_light() +
#   coord_flip()
# 
# ##### The other interaction # VI-based interaction statistic
# int.i <- vint(
#   object = gbm.severity,                    # fitted model object
#   feature_names = name_vec,  # features for which to compute pairwise interactions statistics
#   n.trees = gbm.severity$n.trees,                 # needed if object is of class "gbm"
#  
# )
# 
# pint2 <- ggplot(int.i[1:10, ], aes(reorder(Variables, Interaction), Interaction)) +
#   geom_col(width = 0.75) +
#   labs(x = "", y = "Interaction strength", title = "Partial dependence") +
#   theme(axis.text.x = element_text(angle = 45, vjust = 1)) +
#   theme_light() +
#   coord_flip()
# 
# 
# 
# 
# ###### Partial dependency plots 
# gbm.fit.final=gbm.severity
# 
# # gbm.fit.final %>%
# #   partial(pred.var = "Region", n.trees = gbm.fit.final$n.trees, grid.resolution = 100) %>%
# #   autoplot(rug = TRUE, train = train_data_sev) +
# #   scale_y_continuous(labels = scales::dollar)
# #### Severity pdp 
# x11()
# p1 <- partial(gbm.severity, pred.var = "Region",n.trees=gbm.severity$n.trees, plot = TRUE, rug = TRUE)
# 
# p2 <- partial(gbm.severity, pred.var = "DrivAge",n.trees=gbm.severity$n.trees, plot = TRUE, rug = TRUE)
# 
# p3 <- partial(gbm.severity, pred.var = "Area",n.trees=gbm.severity$n.trees, plot = TRUE, rug = TRUE)
# p5 <- partial(gbm.severity, pred.var = "VehPower",n.trees=gbm.severity$n.trees, plot = TRUE, rug = TRUE)
# p6 <- partial(gbm.severity, pred.var = "VehAge",n.trees=gbm.severity$n.trees, plot = TRUE, rug = TRUE)
# p7 <- partial(gbm.severity, pred.var = "DrivAge",n.trees=gbm.severity$n.trees, plot = TRUE, rug = TRUE)
# p8 <- partial(gbm.severity, pred.var = "BonusMalus",n.trees=gbm.severity$n.trees, plot = TRUE, rug = TRUE)
# p9 <- partial(gbm.severity, pred.var = "VehBrand",n.trees=gbm.severity$n.trees, plot = TRUE, rug = TRUE)
# p10 <- partial(gbm.severity, pred.var = "VehGas",n.trees=gbm.severity$n.trees, plot = TRUE, rug = TRUE)
# p11 <- partial(gbm.severity, pred.var = "Density",n.trees=gbm.severity$n.trees, plot = TRUE, rug = TRUE)
# p14 <- partial(gbm.severity, pred.var = "Region",n.trees=gbm.severity$n.trees, plot = TRUE, rug = TRUE)
# 
# 
# #### Frequency pdp
# p2 <- partial(gbm.severity, pred.var = "DrivAge",n.trees=gbm.severity$n.trees, plot = TRUE, rug = TRUE)
# p3 <- partial(gbm.severity, pred.var = "Area",n.trees=gbm.severity$n.trees, plot = TRUE, rug = TRUE)
# p5 <- partial(gbm.severity, pred.var = "VehPower",n.trees=gbm.severity$n.trees, plot = TRUE, rug = TRUE)
# p6 <- partial(gbm.severity, pred.var = "VehAge",n.trees=gbm.severity$n.trees, plot = TRUE, rug = TRUE)
# p7 <- partial(gbm.severity, pred.var = "DrivAge",n.trees=gbm.severity$n.trees, plot = TRUE, rug = TRUE)
# p8 <- partial(gbm.severity, pred.var = "BonusMalus",n.trees=gbm.severity$n.trees, plot = TRUE, rug = TRUE)
# p9 <- partial(gbm.severity, pred.var = "VehBrand",n.trees=gbm.severity$n.trees, plot = TRUE, rug = TRUE)
# p10 <- partial(gbm.severity, pred.var = "VehGas",n.trees=gbm.severity$n.trees, plot = TRUE, rug = TRUE)
# p11 <- partial(gbm.severity, pred.var = "Density",n.trees=gbm.severity$n.trees, plot = TRUE, rug = TRUE)
# p14 <- partial(gbm.severity, pred.var = "Region",n.trees=gbm.severity$n.trees, plot = TRUE, rug = TRUE)
# 
# ####### frequcny plots 
# p21 <- partial(gbm.fit, pred.var = "DrivAge",n.trees=gbm.fit$n.trees, plot = TRUE, rug = TRUE)
# p31 <- partial(gbm.fit, pred.var = "Area",n.trees=gbm.fit$n.trees, plot = TRUE, rug = TRUE)
# p51 <- partial(gbm.fit, pred.var = "VehPower",n.trees=gbm.fit$n.trees, plot = TRUE, rug = TRUE)
# p61 <- partial(gbm.fit, pred.var = "VehAge",n.trees=gbm.fit$n.trees, plot = TRUE, rug = TRUE)
# p71 <- partial(gbm.fit, pred.var = "DrivAge",n.trees=gbm.fit$n.trees, plot = TRUE, rug = TRUE)
# p81 <- partial(gbm.fit, pred.var = "BonusMalus",n.trees=gbm.fit$n.trees, plot = TRUE, rug = TRUE)
# p91 <- partial(gbm.fit, pred.var = "VehBrand",n.trees=gbm.fit$n.trees, plot = TRUE, rug = TRUE)
# p101 <- partial(gbm.fit, pred.var = "VehGas",n.trees=gbm.fit$n.trees, plot = TRUE, rug = TRUE)
# p111 <- partial(gbm.fit, pred.var = "Density",n.trees=gbm.fit$n.trees, plot = TRUE, rug = TRUE)
# x11()
# p141 <- partial(gbm.fit, pred.var = "Region",n.trees=gbm.fit$n.trees, plot = TRUE, rug = TRUE)
# 
# 
# # Other plots 
# 
# plot_gbm(gbm.fit)
# plot_gbm(gbm.severity)
# 
# ##### First interaction
# pd <- partial(gbm.fit, pred.var = c("VehBrand", "VehGas"),n.trees=gbm.fit$n.trees)
# pd$VehBrand=as.numeric(as.factor(pd$VehBrand))
# pd$VehGas=as.numeric(as.factor(pd$VehGas))
# pd$yhat=exp(pd$yhat)
# 
# pdp1 <- plotPartial(pd)
# 
# ###### Second Interaction Vehicle age vs Vehicle brand
# 
# pd2 <- partial(gbm.fit, pred.var = c("VehBrand", "VehAge"),n.trees=gbm.fit$n.trees)
# pd2$VehBrand=as.factor(pd2$VehBrand)
# pd2$VehAge=as.numeric(pd2$VehAge)
# pdp2 <- plotPartial(pd2)
# 
# VehAgeGLM <- cbind(c(0:110), c(1, rep(2,10), rep(3,100)))
# dat2$VehAgeGLM <- as.factor(VehAgeGLM[dat2$VehAge+1,2])
# dat2[,"VehAgeGLM"] <-relevel(dat2[,"VehAgeGLM"], ref="2")
# 
# #### third interaction power and driver age
# pd3 <- partial(gbm.fit, pred.var = c("DrivAge", "VehPower"),n.trees=gbm.fit$n.trees)
# pd3$yhat=exp(pd3$yhat)
# pdp3 <- plotPartial(pd3)
# 
# 
# #####four Vehicle age vs Vehicle gas 
# pd4 <- partial(gbm.fit, pred.var = c("VehAge", "VehGas"),n.trees=gbm.fit$n.trees)
# # pd4$VehGas=as.factor(pd4$VehGas)
# pd4$yhat=exp(pd4$yhat)
# 
# pd4$VehGas[pd4$VehGas=="Regular"]=1
# pd4$VehGas[pd4$VehGas=="Diesel"]=0
# pd4$VehGas=as.numeric(pd4$VehGas)
# pdp4 <- plotPartial(pd4)
# 
# 
# #### vehicle age and bonus malus
# pd5 <- partial(gbm.fit, pred.var = c("VehAge", "BonusMalus"),n.trees=gbm.fit$n.trees)
# # pd4$VehGas=as.factor(pd4$VehGas)
# pd5$yhat=exp(pd5$yhat)
# 
# pdp5 <- plotPartial(pd5)
# 
# #########driver age and region 
# pd6 <- partial(gbm.fit, pred.var = c("DrivAge", "Region"),n.trees=gbm.fit$n.trees)
# # pd4$VehGas=as.factor(pd4$VehGas)
# pd6$yhat=exp(pd6$yhat)
# pd6$Region=(as.factor(pd6$Region))
# pdp6 <- plotPartial(pd6)
# 
# 
# #####  d.age and bonus malus 
# 
# pd7 <- partial(gbm.fit, pred.var = c("DrivAge", "BonusMalus"),n.trees=gbm.fit$n.trees)
# pd7$yhat=exp(pd7$yhat)
# pdp7 <- plotPartial(pd7)
# 
# ##### Veh power and veh age 
#   
# pd8 <- partial(gbm.fit, pred.var = c("VehPower", "VehAge"),n.trees=gbm.fit$n.trees)
# pd8$yhat=exp(pd8$yhat)
# pdp8 <- plotPartial(pd8)
# 
# ##### bm and region 
# 
# pd8 <- partial(gbm.fit, pred.var = c("BonusMalus", "Region"),n.trees=gbm.fit$n.trees)
# pd8$yhat=exp(pd8$yhat)
# pd8$Region=as.numeric(as.factor(pd8$Region))
# pdp8 <- plotPartial(pd8)
# 
# ##### Vehicle age and Driver age  
# 
# pd9 <- partial(gbm.fit, pred.var = c("VehAge", "DrivAge"),n.trees=gbm.fit$n.trees)
# pd9$yhat=exp(pd9$yhat)
# pdp9 <- plotPartial(pd9)
# 
# 
# #### Severity plots   #######
# 
# ##### VehBrand vs density
# pd11 <- partial(gbm.severity, pred.var = c("VehBrand", "Density"),n.trees=gbm.severity$n.trees)
# pd11$yhat=exp(pd11$yhat)
# pd11$VehBrand=as.numeric(as.factor(pd11$VehBrand))
# pdp11 <- plotPartial(pd11)
# 
# 
# ##### Vehage vs density
# pd12 <- partial(gbm.severity, pred.var = c("VehAge", "Density"),n.trees=gbm.severity$n.trees)
# pd12$yhat=exp(pd12$yhat)
# pdp12 <- plotPartial(pd12)
# 
# ##### Veh age vs driver age
# pd13 <- partial(gbm.severity, pred.var = c("VehAge", "DrivAge"),n.trees=gbm.severity$n.trees)
# pd13$yhat=exp(pd13$yhat)
# pdp13 <- plotPartial(pd13)
# 
# 
# ##### area vs driver age
# pd14 <- partial(gbm.severity, pred.var = c("Area", "DrivAge"),n.trees=gbm.severity$n.trees)
# pd14$yhat=exp(pd14$yhat)
# pd14$Area=as.numeric(as.factor(pd14$Area))
# pdp14 <- plotPartial(pd14)
# 
# ##### region vs driver age
# pd15 <- partial(gbm.severity, pred.var = c( "DrivAge","Region"),n.trees=gbm.severity$n.trees)
# pd15$yhat=exp(pd15$yhat)
# pd15$Region=as.numeric(as.factor(pd15$Region))
# pdp15 <- plotPartial(pd15)
# 
# ##### region vs brand
# pd16 <- partial(gbm.severity, pred.var = c( "VehBrand","Region"),n.trees=gbm.severity$n.trees)
# pd16$yhat=exp(pd16$yhat)
# pd16$Region=as.numeric(as.factor(pd16$Region))
# pd16$VehBrand=as.numeric(as.factor(pd16$VehBrand))
# pdp16 <- plotPartial(pd16)
# 
# ##### driverage vs brand
# pd17 <- partial(gbm.severity, pred.var = c( "DrivAge","VehBrand"),n.trees=gbm.severity$n.trees)
# pd17$yhat=exp(pd17$yhat)
# pd17$VehBrand=as.numeric(as.factor(pd17$VehBrand))
# pdp17 <- plotPartial(pd17)
# 
# ##### bonus vs region 
# pd18 <- partial(gbm.severity, pred.var = c( "BonusMalus","Region"),n.trees=gbm.severity$n.trees)
# pd18$yhat=exp(pd18$yhat)
# pd18$Region=as.numeric(as.factor(pd18$Region))
# pdp18 <- plotPartial(pd18)
# 
# ##### POWER VS brand 
# pd19 <- partial(gbm.severity, pred.var = c( "VehPower","VehBrand"),n.trees=gbm.severity$n.trees)
# pd19$yhat=exp(pd19$yhat)
# pd19$VehBrand=as.numeric(as.factor(pd19$VehBrand))
# pdp19 <- plotPartial(pd19)
# 
# 
# ##### area vs vehicle age  
# pd20 <- partial(gbm.severity, pred.var = c( "Area","VehAge"),n.trees=gbm.severity$n.trees)
# pd20$yhat=exp(pd20$yhat)
# pd20$Area=as.numeric(as.factor(pd20$Area))
# pdp20 <- plotPartial(pd20)
# 
# # Add contour lines and use a different color palette
# rwb <- colorRampPalette(c("red", "white", "blue"))
# pdp3.1 <- plotPartial(pd3, contour = TRUE, col.regions = rwb)
# 
# 
# # 3-D surface
# pdp4.2 <- plotPartial(pd4, levelplot = FALSE, zlab = "cmedv", drape = TRUE,
#                     colorkey = TRUE, screen = list(z = -20, x = -60))
# # Figure 3
# grid.arrange(pdp1, pdp2, pdp3, ncol = 3)

